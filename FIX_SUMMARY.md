# MTUP Training Fixes - Comprehensive Summary

Generated: 2025-12-28
Status: Ready for implementation

## üìä Error Analysis from Evaluation (F1=0.4751)

### Error Distribution:
- **Total examples**: 150
- **Successfully processed**: 103 (68.7%)
- **Errors**: 47 (31.3%)

### Error Categories:

1. **Unmatched Parenthesis** (~25 cases, 53%)
   - Cause: Model generates extra or missing `)`
   - Example: `...:quant 40))))` - 3 extra closing parens

2. **Duplicate Node Names** (~10 cases, 21%)
   - Cause: Model reuses variable names incorrectly
   - Example: `(a / and :op1(...) :op2(a / something))` - `a` appears twice

3. **Template Leakage** (~3 cases, 6%)
   - Cause: Model outputs prompt template text
   - Example: `(2 b∆∞·ªõc)` appears in AMR output

4. **Syntax Errors** (~9 cases, 19%)
   - Various parsing errors, malformed structures

## üîß Root Causes & Solutions

### Issue 1: Template Text Leakage
**Problem**: Template contains `"(2 b∆∞·ªõc)"` which model learns to output

**Current Template (v2_natural)**:
```python
"""### NHI·ªÜM V·ª§
Chuy·ªÉn ƒë·ªïi c√¢u ti·∫øng Vi·ªát sang AMR (2 b∆∞·ªõc)  # ‚Üê PROBLEM: "(2 b∆∞·ªõc)" gets learned
```

**Solution**: Remove parentheses from descriptive text
```python
"""### NHI·ªÜM V·ª§
Chuy·ªÉn ƒë·ªïi c√¢u ti·∫øng Vi·ªát sang AMR theo 2 b∆∞·ªõc  # ‚úì No parentheses
```

### Issue 2: Model Saves to Wrong Path
**Problem**: Training code has bug in save path

**Current Code (train_mtup.py:489)**:
```python
final_model_path = CHECKPOINT_DIR / f"mtup_{args.use_case}_final"
```

But `CHECKPOINT_DIR = OUTPUT_DIR / "checkpoints_mtup"` where `OUTPUT_DIR` is already `PROJECT_ROOT / "outputs"`, causing double `outputs/outputs/` path.

**Solution**: Fix path construction in config_mtup.py

### Issue 3: No Validation During Training
**Problem**: Model doesn't validate output structure during training

**Solution**: Keep end-to-end approach but improve:
1. Better training data validation
2. Cleaner prompts
3. More balanced training (longer training with early stopping)

## üìù Implementation Plan

### Step 1: Fix Prompt Template ‚úÖ
**File**: `config/prompt_templates.py`

**Changes**:
```python
# Line 34-53: Update MTUP_TEMPLATE_V2_NATURAL
MTUP_TEMPLATE_V2_NATURAL = """### NHI·ªÜM V·ª§
Chuy·ªÉn ƒë·ªïi c√¢u ti·∫øng Vi·ªát sang bi·ªÉu di·ªÖn AMR

### C√ÇU ƒê·∫¶U V√ÄO
{sentence}

### K·∫æT QU·∫¢

## B∆Ø·ªöC 1: C·∫•u tr√∫c AMR kh√¥ng c√≥ bi·∫øn
{amr_no_vars}

## B∆Ø·ªöC 2: AMR ho√†n ch·ªânh v·ªõi bi·∫øn

Quy t·∫Øc g√°n bi·∫øn:
- M·ªói kh√°i ni·ªám ‚Üí m·ªôt bi·∫øn duy nh·∫•t
- Kh√°i ni·ªám l·∫∑p l·∫°i ‚Üí d√πng chung bi·∫øn
- Format: (bi·∫øn / kh√°i_ni·ªám :quan_h·ªá ...)

AMR cu·ªëi c√πng:
{amr_with_vars}"""
```

**Key changes**:
- Removed "(2 b∆∞·ªõc)" ‚Üí "theo 2 b∆∞·ªõc" (no parens)
- Clearer section headers
- More explicit variable rules
- "AMR cu·ªëi c√πng" instead of "AMR ho√†n ch·ªânh" (clearer endpoint)

### Step 2: Fix Model Save Path ‚úÖ
**File**: `config/config_mtup.py`

**Current (Line 10-14)**:
```python
PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / "data"
OUTPUT_DIR = PROJECT_ROOT / "outputs"
LOG_DIR = PROJECT_ROOT / "logs"
CHECKPOINT_DIR = OUTPUT_DIR / "checkpoints_mtup"
```

**Problem**: When train_mtup.py uses `CHECKPOINT_DIR`, it creates:
```
outputs/checkpoints_mtup/  # Correct
But actually saves to:
outputs/outputs/checkpoints_mtup/  # Wrong!
```

**Solution**: Verify paths are absolute and correct

**File**: `train_mtup.py` (Line 489-493)

**Change**:
```python
# Before
final_model_path = CHECKPOINT_DIR / f"mtup_{args.use_case}_final"

# After - ensure path is correct
final_model_path = CHECKPOINT_DIR / f"mtup_{args.use_case}_final"
final_model_path.parent.mkdir(parents=True, exist_ok=True)  # ‚Üê Add this
logger.info(f"Saving to: {final_model_path.absolute()}")     # ‚Üê Add logging
```

### Step 3: Improve Training Configuration ‚úÖ
**File**: `config/config_mtup.py`

**Current**:
```python
TRAINING_CONFIG = {
    "num_train_epochs": 10,
    "save_total_limit": 3,  # Only keeps 3 checkpoints
}
```

**Improved**:
```python
TRAINING_CONFIG = {
    "num_train_epochs": 15,  # More epochs for better convergence
    "save_total_limit": 5,   # Keep more checkpoints
    "save_steps": 200,       # Save more frequently
    "eval_steps": 200,       # Evaluate more often
    "load_best_model_at_end": True,  # Load best checkpoint
    "metric_for_best_model": "eval_loss",
}
```

### Step 4: Add Training Validation ‚úÖ
**File**: `train_mtup.py`

**Add after training completes**:
```python
# After line 493 (trainer.save_model)
logger.info(f"\nüíæ Model saved to: {final_model_path}")

# Verify model files exist
required_files = ["adapter_model.safetensors", "adapter_config.json", "tokenizer_config.json"]
missing = []
for file in required_files:
    if not (final_model_path / file).exists():
        missing.append(file)

if missing:
    logger.error(f"‚ùå Missing files after save: {missing}")
    logger.error(f"‚ùå Model may not have saved correctly!")
else:
    logger.info(f"‚úÖ All required files present")
    logger.info(f"‚úÖ Model ready for evaluation/upload")
```

## üéØ Expected Improvements

### From Template Fix:
- **Template leakage**: 3 errors ‚Üí 0 errors (-6%)
- **Clearer prompts**: Better model understanding

### From Better Training:
- **More epochs**: Better convergence
- **More checkpoints**: Can recover best model
- **Validation**: Catch save errors early

### From Path Fix:
- **Model actually saves**: 100% vs 0% currently
- **Can push to HuggingFace**: Critical for deployment

## üìã Next Steps

1. ‚úÖ Apply all fixes to code
2. ‚úÖ Commit changes to git
3. ‚úÖ Train new MTUP model (15 epochs, ~9 hours)
4. ‚úÖ Verify model saves correctly
5. ‚úÖ Evaluate and compare with baseline
6. ‚úÖ Push to HuggingFace Hub

## üîç Monitoring During Training

Watch for these signs of success:

```bash
# Training should show:
‚úì Saving to: /mnt/.../outputs/checkpoints_mtup/mtup_full_training_final
‚úì All required files present
‚úì Model ready for evaluation/upload

# After training:
ls -lh outputs/checkpoints_mtup/mtup_full_training_final/
# Should show:
adapter_model.safetensors  # ~945MB for 3B model
adapter_config.json
tokenizer_config.json
...
```

## üìä Target Metrics

**Current** (with bugs):
- F1: 0.4751
- Errors: 47/150 (31%)
- Model saves: 0% (not saving)

**Target** (after fixes):
- F1: 0.50-0.52 (+5-10%)
- Errors: <35/150 (<23%)
- Model saves: 100%
- Template leakage: 0%

---

**Status**: Ready for implementation
**Estimated improvement**: +5-10% F1, 100% model save reliability
**Training time**: ~9 hours (15 epochs √ó 3B model)
