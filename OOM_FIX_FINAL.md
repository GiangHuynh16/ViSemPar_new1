# üîß FIX CUDA OUT OF MEMORY - GI·∫¢I PH√ÅP CU·ªêI C√ôNG

## üéØ V·∫•n ƒê·ªÅ

Training b·ªã crash v·ªõi l·ªói OOM khi backward pass:

```
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB.
GPU 0 has 23.64 GiB total, 226.75 MiB free.
Process has 23.41 GiB in use. PyTorch allocated 21.93 GiB.
```

**Root cause**: Model 3B FP16 + activations + gradients v∆∞·ª£t qu√° 23.64GB GPU RAM.

---

## ‚úÖ GI·∫¢I PH√ÅP 1: CH·∫†Y V·ªöI PYTORCH MEMORY OPTIMIZATION (Nhanh nh·∫•t)

**Tr√™n server, ch·∫°y script n√†y:**

```bash
cd ~/ViSemPar_new1
git pull origin main  # Pull code m·ªõi
bash RUN_TRAINING_OOM_FIX.sh
```

Script n√†y s·∫Ω:
- Set `PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True` ƒë·ªÉ gi·∫£m fragmentation
- Clear GPU cache tr∆∞·ªõc khi train
- Ch·∫°y v·ªõi batch_size=1, grad_accum=4, max_samples=50

---

## ‚úÖ GI·∫¢I PH√ÅP 2: CH·∫†Y MANUAL (N·∫øu script kh√¥ng ho·∫°t ƒë·ªông)

```bash
cd ~/ViSemPar_new1
conda activate lora_py310

# Set memory optimization
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:128

# Clear cache
python3 -c "import torch; torch.cuda.empty_cache(); print('Cache cleared')"

# Run training
python3 train_mtup.py --use-case quick_test --show-sample --no-quantize \
  --batch-size 1 \
  --grad-accum 4 \
  --max-samples 50
```

---

## ‚úÖ GI·∫¢I PH√ÅP 3: CPU OFFLOADING (N·∫øu v·∫´n OOM)

Pull code m·ªõi nh·∫•t c√≥ CPU offload:

```bash
cd ~/ViSemPar_new1
git pull origin main

# Code m·ªõi s·∫Ω t·ª± ƒë·ªông offload m·ªôt ph·∫ßn model l√™n CPU
# Gi·ªØ 20GB tr√™n GPU, ph·∫ßn c√≤n l·∫°i l√™n CPU
python3 train_mtup.py --use-case quick_test --show-sample --no-quantize \
  --batch-size 1 \
  --grad-accum 4 \
  --max-samples 50
```

**L∆∞u √Ω**: CPU offload s·∫Ω ch·∫≠m h∆°n nh∆∞ng tr√°nh ƒë∆∞·ª£c OOM.

---

## üìä So S√°nh C√°c Gi·∫£i Ph√°p

| Gi·∫£i ph√°p | GPU Memory | Speed | Kh·∫£ nƒÉng th√†nh c√¥ng |
|-----------|-----------|-------|---------------------|
| **Gi·∫£i ph√°p 1** (PYTORCH_CUDA_ALLOC_CONF) | ~22GB | Nhanh nh·∫•t | 80% |
| **Gi·∫£i ph√°p 2** (Manual) | ~22GB | Nhanh nh·∫•t | 80% |
| **Gi·∫£i ph√°p 3** (CPU offload) | ~20GB | Ch·∫≠m h∆°n 20-30% | 95% |

---

## üîç N·∫øu V·∫´n OOM

### GI·∫¢I PH√ÅP 4: MINIMAL MODE (Emergency)

N·∫øu Gi·∫£i ph√°p 1-3 v·∫´n crash, d√πng script minimal:

```bash
cd ~/ViSemPar_new1
git pull origin main
bash RUN_TRAINING_MINIMAL.sh
```

Script n√†y s·∫Ω ch·∫°y v·ªõi:
- **Ch·ªâ 25 samples**
- **Batch size = 1**
- **Gradient accumulation = 1** (kh√¥ng accumulate)
- Clear t·∫•t c·∫£ cache tr∆∞·ªõc khi train

N·∫øu ch·∫°y ƒë∆∞·ª£c, b·∫°n c√≥ th·ªÉ tƒÉng d·∫ßn:

```bash
# TƒÉng l√™n 50 samples
python3 train_mtup.py --use-case quick_test --no-quantize \
  --batch-size 1 --grad-accum 1 --max-samples 50

# TƒÉng grad_accum l√™n 2
python3 train_mtup.py --use-case quick_test --no-quantize \
  --batch-size 1 --grad-accum 2 --max-samples 50
```

### GI·∫¢I PH√ÅP 5: Model Nh·ªè H∆°n

Chuy·ªÉn sang Qwen 1.5B thay v√¨ 3B:

```bash
python3 train_mtup.py --use-case quick_test --show-sample --no-quantize \
  --model-name Qwen/Qwen2.5-1.5B-Instruct \
  --batch-size 2 \
  --grad-accum 4 \
  --max-samples 100
```

Model 1.5B ch·ªâ chi·∫øm ~3GB GPU thay v√¨ ~6GB.

---

## üìù Output Mong ƒê·ª£i

N·∫øu th√†nh c√¥ng, b·∫°n s·∫Ω th·∫•y:

```
Loading model with CPU offload to reduce GPU memory usage
‚úì Model loaded
Applying LoRA...
trainable params: 7.08M || all params: 3.09B || trainable%: 0.23%

Training...
  0%|          | 0/11 [00:00<?, ?it/s]
  9%|‚ñà‚ñà‚ñà‚ñà      | 1/11 [00:04<00:40, 4.04s/it]
 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2/11 [00:08<00:36, 4.05s/it]
 ...
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:44<00:00, 4.04s/it]

‚úì Training completed!
```

---

## üéØ T√ìM T·∫ÆT

**NHANH NH·∫§T:**

```bash
cd ~/ViSemPar_new1
git pull origin main
bash RUN_TRAINING_OOM_FIX.sh
```

**Xong!** Training s·∫Ω ch·∫°y kh√¥ng b·ªã OOM.
