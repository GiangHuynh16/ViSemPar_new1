# ğŸ”§ TÃ³m táº¯t: CÃ¡c lá»—i training vÃ  cÃ¡ch fix

## ğŸš¨ 3 lá»—i nghiÃªm trá»ng phÃ¡t hiá»‡n

### 1. âŒ Thiáº¿u EOS Token

**Váº¥n Ä‘á»:**
```python
# Code cÅ© (SAI):
text = PROMPT_TEMPLATE.format(sentence=sentence) + amr
```

Model khÃ´ng biáº¿t khi nÃ o dá»«ng â†’ Sinh thÃªm text sau AMR

**Evidence tá»« output:**
- Model sinh ra giáº£i thÃ­ch sau AMR
- CÃ¢u vÄƒn tiáº¿p tá»¥c sau khi AMR Ä‘Ã£ káº¿t thÃºc
- KhÃ´ng cÃ³ Ä‘iá»ƒm dá»«ng rÃµ rÃ ng

**Fix:**
```python
# Code má»›i (ÄÃšNG):
full_text = prompt + amr + tokenizer.eos_token
```

---

### 2. âŒ KhÃ´ng cÃ³ Instruction Masking

**Váº¥n Ä‘á»:**
```python
# Code cÅ© (SAI):
labels = input_ids.clone()
labels[labels == tokenizer.pad_token_id] = -100
```

Model há»c cáº£ prompt "Báº¡n lÃ  chuyÃªn gia..." â†’ LÃ£ng phÃ­!

**Táº¡i sao sai:**
- Model pháº£i há»c cáº£ instruction (khÃ´ng cáº§n thiáº¿t)
- Cháº­m há»™i tá»¥
- Loss cao á»Ÿ pháº§n khÃ´ng cáº§n há»c

**Fix:**
```python
# Code má»›i (ÄÃšNG):
# 1. TÃ¡ch prompt vÃ  AMR
prompt = example['prompt']
amr = example['amr']

# 2. TÃ­nh prompt length
prompt_encoding = tokenizer(prompt, ...)
prompt_length = len(prompt_encoding['input_ids'][0])

# 3. Mask instruction part
labels = input_ids.clone()
labels[:prompt_length] = -100  # â† CRITICAL FIX
labels[labels == tokenizer.pad_token_id] = -100
```

**Giáº£i thÃ­ch:**
- `-100` = ignore trong loss calculation
- Chá»‰ train trÃªn AMR output (pháº§n sau prompt)
- Model há»c nhanh hÆ¡n, hiá»‡u quáº£ hÆ¡n

---

### 3. âŒ Prompt khÃ´ng rÃµ rÃ ng vá» Penman format

**Prompt cÅ© (THIáº¾U):**
```
Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch ngá»¯ nghÄ©a tiáº¿ng Viá»‡t.
HÃ£y chuyá»ƒn Ä‘á»•i cÃ¢u sau sang Ä‘á»‹nh dáº¡ng AMR.

Quy táº¯c:
- Sá»­ dá»¥ng khÃ¡i niá»‡m tiáº¿ng Viá»‡t cÃ³ dáº¥u gáº¡ch dÆ°á»›i
- GÃ¡n biáº¿n cho má»—i khÃ¡i niá»‡m
- ... (mÆ¡ há»“)

CÃ¢u: {sentence}
AMR:
```

**Váº¥n Ä‘á»:**
- KhÃ´ng nÃ³i rÃµ "Ä‘á»‹nh dáº¡ng Penman"
- KhÃ´ng cáº¥m giáº£i thÃ­ch
- KhÃ´ng nháº¥n máº¡nh cáº¥u trÃºc cÃ¢y

**Prompt má»›i (RÃ• RÃ€NG):**
```
Báº¡n lÃ  chuyÃªn gia ngÃ´n ngá»¯ há»c mÃ¡y tÃ­nh, chuyÃªn vá» phÃ¢n tÃ­ch ngá»¯ nghÄ©a tiáº¿ng Viá»‡t.
HÃ£y chuyá»ƒn Ä‘á»•i cÃ¢u vÄƒn sau sang Ä‘á»‹nh dáº¡ng AMR theo Ä‘Ãºng **chuáº©n Penman**.

CÃ¡c quy táº¯c báº¯t buá»™c:
1. Sá»­ dá»¥ng Ä‘á»‹nh dáº¡ng Penman: (biáº¿n / khÃ¡i niá»‡m :quan-há»‡ (biáº¿n2 / khÃ¡i niá»‡m2))
2. KhÃ¡i niá»‡m tiáº¿ng Viá»‡t Ä‘a Ã¢m tiáº¿t pháº£i dÃ¹ng dáº¥u gáº¡ch dÆ°á»›i (vÃ­ dá»¥: c / chÃ­nh_phá»§)
3. Sá»­ dá»¥ng cÃ¡c quan há»‡ chuáº©n: :ARG0, :ARG1, :ARG2, :time, :location, ...
4. Äáº£m báº£o cáº¥u trÃºc cÃ¢y vá»›i ngoáº·c Ä‘Æ¡n hoÃ n toÃ n cÃ¢n báº±ng
5. Má»—i khÃ¡i niá»‡m chá»‰ Ä‘Æ°á»£c gÃ¡n má»™t biáº¿n duy nháº¥t
6. KHÃ”NG thÃªm giáº£i thÃ­ch, chá»‰ tráº£ vá» cáº¥u trÃºc AMR thuáº§n tÃºy  â† CRITICAL

CÃ¢u tiáº¿ng Viá»‡t: {sentence}

AMR (Penman):
```

**Äiá»ƒm khÃ¡c biá»‡t:**
- âœ… NÃ³i rÃµ "chuáº©n Penman"
- âœ… VÃ­ dá»¥ cá»¥ thá»ƒ format
- âœ… Cáº¥m giáº£i thÃ­ch (quy táº¯c #6)
- âœ… Nháº¥n máº¡nh balanced parentheses

---

## ğŸ“Š Káº¿t quáº£ cá»§a cÃ¡c lá»—i nÃ y

### Model cÅ© (cÃ³ 3 lá»—i):
- âŒ 26/150 AMR khÃ´ng há»£p lá»‡ (17.3% lá»—i)
- âŒ Unmatched parentheses
- âŒ Duplicate node names
- âŒ Sinh giáº£i thÃ­ch sau AMR
- âŒ KhÃ´ng thá»ƒ tÃ­nh SMATCH

### Model má»›i (sau fix):
- âœ… Dá»± kiáº¿n 150/150 AMR há»£p lá»‡
- âœ… Balanced parentheses
- âœ… KhÃ´ng duplicate nodes
- âœ… Dá»«ng Ä‘Ãºng táº¡i EOS
- âœ… TÃ­nh Ä‘Æ°á»£c SMATCH

---

## ğŸ”§ Chi tiáº¿t implementation

### File cáº§n sá»­a:

1. **config/config_fixed.py** â† Prompt má»›i
2. **train_baseline_fixed.py** â† Training script má»›i vá»›i masking
3. **predict_baseline_fixed.py** â† Prediction script

### Dataset class má»›i:

```python
class BaselineDatasetFixed(Dataset):
    def __getitem__(self, idx):
        example = self.examples[idx]

        prompt = example['prompt']
        amr = example['amr']

        # FIX 1: Add EOS token
        full_text = prompt + amr + self.tokenizer.eos_token

        # Tokenize
        encoding = self.tokenizer(full_text, ...)
        input_ids = encoding['input_ids'].squeeze()
        labels = input_ids.clone()

        # FIX 2: Instruction masking
        prompt_encoding = self.tokenizer(prompt, ...)
        prompt_length = len(prompt_encoding['input_ids'][0])
        labels[:prompt_length] = -100  # Mask instruction

        # Mask padding
        labels[labels == self.tokenizer.pad_token_id] = -100

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels
        }
```

### Generation config:

```python
outputs = model.generate(
    **inputs,
    max_new_tokens=512,
    temperature=0.1,
    top_p=0.9,
    repetition_penalty=1.15,
    eos_token_id=tokenizer.eos_token_id,  # â† CRITICAL
    pad_token_id=tokenizer.pad_token_id,
)

# Remove EOS and explanations
generated = tokenizer.decode(outputs[0])
amr = generated[len(prompt):].strip()
if tokenizer.eos_token in amr:
    amr = amr.split(tokenizer.eos_token)[0].strip()
```

---

## ğŸ“‹ Checklist Ä‘á»ƒ verify fixes

Sau khi train xong, check:

- [ ] **EOS Token:**
  - [ ] Training data cÃ³ thÃªm `tokenizer.eos_token` chÆ°a?
  - [ ] Generation config cÃ³ `eos_token_id` chÆ°a?
  - [ ] Output cÃ³ dá»«ng Ä‘Ãºng khÃ´ng?

- [ ] **Instruction Masking:**
  - [ ] Labels cÃ³ mask prompt (`labels[:prompt_length] = -100`)?
  - [ ] Training loss chá»‰ tÃ­nh trÃªn AMR?
  - [ ] Check sample batch xem labels Ä‘Ãºng chÆ°a

- [ ] **Prompt:**
  - [ ] CÃ³ tá»« "Penman" trong prompt?
  - [ ] CÃ³ cáº¥m giáº£i thÃ­ch (quy táº¯c #6)?
  - [ ] CÃ³ vÃ­ dá»¥ cá»¥ thá»ƒ format?

- [ ] **Output Quality:**
  - [ ] Táº¥t cáº£ AMR cÃ³ balanced parentheses?
  - [ ] KhÃ´ng cÃ³ duplicate nodes?
  - [ ] KhÃ´ng cÃ³ giáº£i thÃ­ch sau AMR?
  - [ ] SMATCH tÃ­nh Ä‘Æ°á»£c?

---

## ğŸ¯ Commands Ä‘á»ƒ cháº¡y

```bash
# 1. Cleanup
chmod +x CLEANUP_AND_ORGANIZE.sh
./CLEANUP_AND_ORGANIZE.sh

# 2. Train vá»›i fixes
conda activate baseline_final
python train_baseline_fixed.py --epochs 15 --show-sample

# 3. Archive model
TIMESTAMP=$(ls -t outputs/ | grep baseline_fixed | head -1)
mv outputs/$TIMESTAMP/final models_archive/baseline_7b_fixed/

# 4. Test
python predict_baseline_fixed.py \
  --model models_archive/baseline_7b_fixed/final \
  --test-file data/public_test.txt \
  --output evaluation_results/baseline_7b_fixed/predictions.txt

# 5. Evaluate
python -m smatch -f \
  evaluation_results/baseline_7b_fixed/predictions.txt \
  data/public_test_ground_truth.txt \
  --significant 4
```

---

## ğŸ’¡ Táº¡i sao nhá»¯ng lá»—i nÃ y quan trá»ng?

### 1. EOS Token:
- KhÃ´ng cÃ³ â†’ Model khÃ´ng biáº¿t dá»«ng
- CÃ³ â†’ Model dá»«ng Ä‘Ãºng lÃºc
- **Impact:** Tá»« "sinh vÃ´ táº­n" â†’ "sinh Ä‘Ãºng Ä‘á»™ dÃ i"

### 2. Instruction Masking:
- KhÃ´ng cÃ³ â†’ Model há»c cáº£ prompt
- CÃ³ â†’ Model chá»‰ há»c AMR
- **Impact:** Tá»« "cháº­m, lÃ£ng phÃ­" â†’ "nhanh, hiá»‡u quáº£"

### 3. Clear Prompt:
- KhÃ´ng rÃµ â†’ Model Ä‘oÃ¡n format
- RÃµ rÃ ng â†’ Model biáº¿t chÃ­nh xÃ¡c
- **Impact:** Tá»« "17% lá»—i" â†’ "0% lá»—i" (dá»± kiáº¿n)

---

## ğŸ“š References

### Files created:
1. `config/config_fixed.py` - Fixed config with new prompt
2. `train_baseline_fixed.py` - Fixed training script
3. `predict_baseline_fixed.py` - Fixed prediction script
4. `CLEANUP_AND_ORGANIZE.sh` - Cleanup script
5. `RETRAIN_INSTRUCTIONS.md` - Detailed instructions
6. `TRAINING_FIXES_SUMMARY.md` - This file

### Next steps:
1. Read `RETRAIN_INSTRUCTIONS.md`
2. Run cleanup
3. Train with `train_baseline_fixed.py`
4. Evaluate and compare

---

**Author:** Claude Code
**Date:** 2026-01-03
**Version:** 1.0
