# ğŸ”§ Retrain Baseline 7B - FIXED VERSION

## âš ï¸ Váº¥n Ä‘á» vá»›i model cÅ©

Model baseline 7B Ä‘áº§u tiÃªn cÃ³ **3 lá»—i nghiÃªm trá»ng**:

1. **âŒ Thiáº¿u EOS token**
   - Model khÃ´ng biáº¿t khi nÃ o dá»«ng generate
   - â†’ Sinh ra text vÃ´ táº­n, thÃªm giáº£i thÃ­ch sau AMR

2. **âŒ KhÃ´ng mask instruction**
   - Model há»c cáº£ pháº§n prompt "Báº¡n lÃ  chuyÃªn gia..."
   - â†’ LÃ£ng phÃ­ capacity, cháº­m há»™i tá»¥

3. **âŒ Prompt khÃ´ng rÃµ format Penman**
   - Model khÃ´ng hiá»ƒu cáº¥u trÃºc AMR chuáº©n
   - â†’ 26/150 AMR khÃ´ng há»£p lá»‡ (17.3% lá»—i)

## âœ… CÃ¡c fix Ä‘Ã£ Ã¡p dá»¥ng

### 1. ThÃªm EOS Token
```python
# OLD (SAI):
text = PROMPT_TEMPLATE.format(sentence=sentence) + amr

# NEW (ÄÃšNG):
full_text = prompt + amr + tokenizer.eos_token
```

### 2. Instruction Masking
```python
# Chá»‰ train trÃªn AMR output, khÃ´ng train trÃªn instruction
prompt_length = len(prompt_encoding['input_ids'][0])
labels[:prompt_length] = -100  # Mask instruction part
```

### 3. Prompt rÃµ rÃ ng vá» Penman
```
CÃ¡c quy táº¯c báº¯t buá»™c:
1. Sá»­ dá»¥ng Ä‘á»‹nh dáº¡ng Penman: (biáº¿n / khÃ¡i niá»‡m :quan-há»‡ ...)
2. KhÃ¡i niá»‡m tiáº¿ng Viá»‡t Ä‘a Ã¢m tiáº¿t pháº£i dÃ¹ng dáº¥u gáº¡ch dÆ°á»›i
3. Sá»­ dá»¥ng cÃ¡c quan há»‡ chuáº©n: :ARG0, :ARG1, :ARG2, ...
4. Äáº£m báº£o cáº¥u trÃºc cÃ¢y vá»›i ngoáº·c Ä‘Æ¡n cÃ¢n báº±ng
5. Má»—i khÃ¡i niá»‡m chá»‰ Ä‘Æ°á»£c gÃ¡n má»™t biáº¿n duy nháº¥t
6. KHÃ”NG thÃªm giáº£i thÃ­ch, chá»‰ tráº£ vá» AMR thuáº§n tÃºy
```

## ğŸ“‹ BÆ°á»›c thá»±c hiá»‡n

### BÆ°á»›c 1: Cleanup files cÅ©

```bash
cd /mnt/nghiepth/giangha/visempar/ViSemPar_new1

# Run cleanup script
chmod +x CLEANUP_AND_ORGANIZE.sh
./CLEANUP_AND_ORGANIZE.sh
```

**Káº¿t quáº£:**
- Model cÅ© â†’ `models_archive/baseline_7b_old/` (archived)
- Results cÅ© â†’ `evaluation_results/baseline_7b_old/`
- Temp files â†’ `temp_files/`

### BÆ°á»›c 2: Activate conda environment

```bash
conda activate baseline_final
```

### BÆ°á»›c 3: Train model vá»›i fixes

```bash
python train_baseline_fixed.py --epochs 15 --show-sample
```

**Training sáº½:**
- âœ… Sá»­ dá»¥ng `config_fixed.py` vá»›i prompt má»›i
- âœ… ThÃªm EOS token vÃ o má»—i example
- âœ… Mask instruction (chá»‰ train trÃªn AMR)
- âœ… LÆ°u checkpoint vÃ o `outputs/baseline_fixed_YYYYMMDD_HHMMSS/`

**Expected output:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     VIETNAMESE AMR PARSER - BASELINE TRAINING FIXED         â•‘
â•‘     âœ… EOS Token | âœ… Instruction Masking | âœ… Penman      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

APPLYING FIXES:
  1. Adding EOS token to each example
  2. Preparing for instruction masking
  3. Using clear Penman format prompt

SAMPLE FIXED EXAMPLE:
PROMPT (will be masked):
Báº¡n lÃ  chuyÃªn gia ngÃ´n ngá»¯ há»c mÃ¡y tÃ­nh...
...
AMR (will be trained):
(t / tuyÃªn_bá»‘ :ARG0 ...)
```

### BÆ°á»›c 4: Archive model sau khi train xong

```bash
# Sau khi training hoÃ n táº¥t
TIMESTAMP=$(ls -t outputs/ | grep baseline_fixed | head -1)
mv outputs/$TIMESTAMP/final models_archive/baseline_7b_fixed/

# Táº¡o README
cat > models_archive/baseline_7b_fixed/README.md << 'EOF'
# Baseline 7B Model - FIXED VERSION âœ…

**Training Date:** $(date +%Y-%m-%d)
**Status:** âœ… All fixes applied

## Fixes:
1. âœ… EOS token added
2. âœ… Instruction masking enabled
3. âœ… Clear Penman format prompt

## Expected Results:
- Should generate valid Penman AMR
- Should stop at EOS token
- Should NOT generate explanations
- Error rate << 17.3% (old version)

## Training:
- Epochs: 15
- Batch size: 1 x 16 (gradient accumulation)
- Learning rate: 2e-4
- LoRA rank: 64
- Dataset: train_amr_1.txt + train_amr_2.txt
EOF
```

### BÆ°á»›c 5: Test vÃ  evaluate

```bash
# Generate predictions
python predict_baseline_fixed.py \
  --model models_archive/baseline_7b_fixed/final \
  --test-file data/public_test.txt \
  --output evaluation_results/baseline_7b_fixed/predictions.txt

# Check quality
python analyze_amr_quality.py \
  --file evaluation_results/baseline_7b_fixed/predictions.txt
```

**Expected:**
- âœ… 150/150 valid AMRs (0% error rate)
- âœ… All AMRs stop at proper endpoint
- âœ… No explanations after AMR
- âœ… SMATCH score calculable

### BÆ°á»›c 6: Compare vá»›i model cÅ©

```bash
echo "OLD MODEL (buggy):"
cat evaluation_results/baseline_7b_old/README.md

echo ""
echo "NEW MODEL (fixed):"
wc -l evaluation_results/baseline_7b_fixed/predictions.txt
python -m smatch -f \
  evaluation_results/baseline_7b_fixed/predictions.txt \
  data/public_test_ground_truth.txt \
  --significant 4
```

## ğŸ“Š Expected Improvements

| Metric | Old (Buggy) | New (Fixed) | Improvement |
|--------|-------------|-------------|-------------|
| Valid AMRs | 124/150 (82.7%) | 150/150 (100%) | +17.3% |
| Parse errors | 26 (17.3%) | 0 (0%) | -100% |
| SMATCH | Not calculable | XX.X% | Measurable! |
| Generates explanations | Yes âŒ | No âœ… | Fixed |
| Stops at EOS | No âŒ | Yes âœ… | Fixed |

## ğŸ¯ Success Criteria

Model Ä‘Æ°á»£c coi lÃ  thÃ nh cÃ´ng khi:
- [ ] 100% AMRs há»£p lá»‡ (balanced parentheses, no duplicates)
- [ ] Model dá»«ng Ä‘Ãºng táº¡i EOS token
- [ ] KhÃ´ng generate giáº£i thÃ­ch sau AMR
- [ ] SMATCH score Ä‘Æ°á»£c tÃ­nh thÃ nh cÃ´ng
- [ ] SMATCH F1 > 0.0 (tá»‘i thiá»ƒu)

## ğŸš¨ Troubleshooting

### Náº¿u váº«n cÃ²n invalid AMRs:
1. Check xem cÃ³ thÃªm EOS token chÆ°a:
   ```python
   # In create_baseline_dataset:
   full_text = prompt + amr + self.tokenizer.eos_token
   ```

2. Check instruction masking:
   ```python
   # Labels should have -100 for instruction part
   labels[:prompt_length] = -100
   ```

3. Check generation config:
   ```python
   # Must have eos_token_id
   outputs = model.generate(
       ...,
       eos_token_id=tokenizer.eos_token_id,
   )
   ```

### Náº¿u model váº«n generate giáº£i thÃ­ch:
1. Check prompt cÃ³ rÃµ "KHÃ”NG thÃªm giáº£i thÃ­ch" chÆ°a
2. Check repetition_penalty (nÃªn lÃ  1.15)
3. Check temperature (nÃªn lÃ  0.1 - deterministic)

## ğŸ“ File Structure sau khi hoÃ n táº¥t

```
ViSemPar_new1/
â”œâ”€â”€ models_archive/
â”‚   â”œâ”€â”€ baseline_7b_old/          âš ï¸  Archived (buggy)
â”‚   â”‚   â”œâ”€â”€ checkpoint-1545/
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ baseline_7b_fixed/        âœ… New (fixed)
â”‚   â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”‚   â”œâ”€â”€ adapter_model.safetensors
â”‚   â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ evaluation_results/
â”‚   â”œâ”€â”€ baseline_7b_old/          Old results
â”‚   â”‚   â”œâ”€â”€ predictions_formatted.txt
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â””â”€â”€ baseline_7b_fixed/        âœ… New results
â”‚       â”œâ”€â”€ predictions.txt
â”‚       â””â”€â”€ smatch_score.txt
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.py                 Old config
â”‚   â””â”€â”€ config_fixed.py           âœ… New config
â”œâ”€â”€ train_baseline.py             Old training script
â”œâ”€â”€ train_baseline_fixed.py       âœ… New training script
â””â”€â”€ predict_baseline_fixed.py     âœ… New prediction script
```

## â±ï¸ Timeline

- Cleanup: ~1 phÃºt
- Training: ~2-3 giá» (15 epochs, 7B model)
- Prediction: ~10-15 phÃºt (150 sentences)
- Evaluation: ~5-10 phÃºt

**Tá»•ng cá»™ng: ~3-4 giá»**

## ğŸ“ Notes

- **CRITICAL:** Pháº£i dÃ¹ng `train_baseline_fixed.py`, KHÃ”NG dÃ¹ng `train_baseline.py`
- **CRITICAL:** Pháº£i import tá»« `config_fixed`, KHÃ”NG import tá»« `config`
- Backup model cÅ© trÆ°á»›c khi cleanup (Ä‘Ã£ lÃ m qua script)
- Monitor training loss - nÃªn giáº£m Ä‘á»u, khÃ´ng NaN
- Check GPU memory - nÃªn stable ~40-45GB

---

**Prepared by:** Claude Code
**Date:** 2026-01-03
**Version:** 1.0 - Fixed Baseline Training
